---
title: "Data-581 Lab 3"
author: "Dhun Sheth"
date: "2023-12-04"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Matrix)

MyText <- readLines("text.txt")
MyTextStrings <- strsplit(MyText, " ")
MyTextStrings <- unlist(MyTextStrings)
lettercounts <- nchar(MyTextStrings)

lettercounts <- lettercounts[lettercounts > 0]

lettercountsT <- lettercounts
lettercountsT[lettercounts > 11] <- 12


P <- matrix(0, nrow=12, ncol=12)
for (i in 2:length(lettercountsT)) {
P[lettercountsT[i-1], lettercountsT[i]] <- P[lettercountsT[i-1],lettercountsT[i]] + 1
}
P <- P/as.numeric(table(lettercountsT[-length(lettercountsT)]))
length(table(lettercountsT[-length(lettercountsT)]))


```

## Part 8 
```{r question_8}

set.seed(8)

wordlengthAVGs <- numeric(1000)
wordlengthSDs <- numeric(1000)
diff9 <- numeric(1000)
for (i in 1:1000) {
Ntransitions <- length(lettercounts) # number of words
wordlength <- numeric(Ntransitions)#initializing the Markov chain
current.state <- lettercountsT[1] # initial wordlength
for (j in 1:Ntransitions) {
current.state <- sample(1:12,
size = 1, prob = P[current.state, ])
wordlength[j] <- current.state
}
wordlengthAVGs[i] <- mean(wordlength)
wordlengthSDs[i] <- sd(wordlength)
diff9[i] <- mean(abs(diff(wordlength)) > 9)
}

par(mfrow = c(2, 2))
hist(wordlengthAVGs, freq=FALSE)
rug(mean(lettercountsT), col=2, lwd=3, ticksize=.5)

hist(wordlengthSDs, freq=FALSE)
rug(sd(lettercountsT), col=2, lwd=3, ticksize=.5)


mean_ind <- numeric(1000)
sd_ind <- numeric(1000)
for (i in 1:1000) {
indepcounts <- sample(1:12, size=length(lettercounts), replace=TRUE, prob=table(lettercountsT)/length(lettercounts))
mean_ind[i] <- mean(indepcounts)
sd_ind[i] <- sd(indepcounts)
}


hist(mean_ind, freq=FALSE)
rug(mean(lettercountsT), col=2, lwd=3, ticksize=.5)

hist(sd_ind, freq=FALSE)
rug(sd(lettercountsT), col=2, lwd=3, ticksize=.5)
par(mfrow = c(1, 1))
```

We would be suspicious of any texts with mean lower than 4.9 or higher than 5.35.  

We would be suspicious of any texts with standard deviation lower than 2.75 or higher than 3.1.  

Important to note here, when the Markov chain model is not used the histograms for mean and standard deviation do not change much. 

## Question 9
### Part D
```{r question_9_part_d}

P <- matrix(
  c(.4+.3,.4,0,0,0,0,0,
    .2,.3,.4,0,0,0,0,
    .1,.2,.3,.4,0,0,0,
    0,.1,.2,.3,.4+.1,0,0,
    0,0,.1,.2,.3,.4+.2+.1,0,
    0,0,0,.1,.2,.3,1,
    0,0,0,0,0,0,0
  ), nrow = 7
)
print(P)
Ntransitions <- 100000
location <- numeric(Ntransitions)#initializing the Markov chain
current.state <- 2 # initial compartment
for (t in 1:Ntransitions) {
current.state <- sample(1:7,
size = 1, prob = P[current.state, ])
location[t] <- current.state
}
print(head(location, n=10))

stationary_distribution <- numeric(dim(P)[1])
for (state in location){
  stationary_distribution[state] <- stationary_distribution[state] + 1
}
stationary_distribution <- stationary_distribution / sum(stationary_distribution)
print(stationary_distribution)

```
In transition matrix P, row is the number of containers at the start of the day and column is the number of containers at the end of the day (after 1 container has been removed). To have 6 containers at the end of the day, we would need total containers during the day to be 7 so 7-1=6, however, the max capacity of stockyard is 6 and if this is exceeded the shipment is not accepted, so it is impossible to have 6 containers at the end of the day, which is why the last column is all 0's. However, it may be possible to start with 6 containers as the starting state and thus would have a probability of 100% to end the day with 5 containers, as no shipments would be accepted during the day and 1 container would be removed, leaving 5 containers at the end of the day. 

## Question 10
### Part A
```{r question_10_part_a}

p <- matrix(c(.5,.25,0,0,.75,1,.5,0,0), nrow=3)
p_2 <- p%*%p
print(p_2)
```


Then $P(X_2=1|X_0=2) = P_{2,1}^{2}$  = `r p_2[2,1]`

### Part B
```{r question_10_part_b}

print(p %*% p %*% p)

```
Because there is a non-zero probability from state 1 to 3 and state 3 to 1 for s.t $P_{ij}^n \gt 0$ for some n and $P_{ji}^n \gt 0$ for some other n, we can conclude States 1 and 3 do communicate with each other.

### Part C
$P(X_5=3|X_0=1)=0.1406 $ from the transition matrix given by $P_{1,3}^5$

### Part D
```{r question_10_part_d}

print(p %*% p %*% p %*% p %*% p %*% p)

```
Because all elements in $P_{ij}^6 \gt 0$, all elements communicate with each other making the transition matrix P itself a class => matrix P is said to be irreducible. In addition, because all elements i, j communicate with each other, they share the same period, => it is aperiodic.    

By Chapman-Kolmogorov Equations Theorem 5: Because P is aperiodic and irreducible it must be a regular transition matrix. 

### Part E
```{r question_10_part_e}

A <- t(p) - diag(rep(1,3))
A <- rbind(A, rep(1,3))
RHS <- c(rep(0,3), 1)
qr.solve(A, RHS)

```
