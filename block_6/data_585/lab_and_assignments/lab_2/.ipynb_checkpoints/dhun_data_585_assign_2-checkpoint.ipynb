{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf74062-f093-4ce1-b0bb-fb0667c78caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from scipy.misc import derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8fcec-e2e7-481e-8daf-c9de36abe505",
   "metadata": {},
   "source": [
    "# DATA-585 Assignment 2\n",
    "\n",
    "## Part I\n",
    "### 1. Finding first derivative and if stationary points can be found analytically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3d9a64-fd95-4933-8f4b-ac83d5b721ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(x) = f'(x) = x**11 + x**5 + exp(x) + 1\n"
     ]
    }
   ],
   "source": [
    "x = sp.Symbol('x')\n",
    "eqn = sp.exp(x) + x**12/12 + x**6/6 + x\n",
    "derivative_x = sp.diff(eqn, x)\n",
    "print(\"g(x) = f'(x) =\", derivative_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ea071-5125-494e-a170-f777ef80f1c6",
   "metadata": {},
   "source": [
    "You cannot find stationary points analytically for the derivative because of the exponential term and high-order polynomial. As such, estimation techniques would need to be used to find the stationary points.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccf221-70fc-42ae-8f74-4188dc953099",
   "metadata": {},
   "source": [
    "### 2. Finding second derivative and if f is convex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e901a1-dc16-44b9-90b1-41d230c6afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x) = f''(x) = 11*x**10 + 5*x**4 + exp(x)\n"
     ]
    }
   ],
   "source": [
    "sec_derivative = sp.diff(derivative_x, x)\n",
    "print(\"H(x) = f''(x) =\", sec_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34c84f-f579-4eb4-9303-82fc686642a6",
   "metadata": {},
   "source": [
    "Because H(x) >= 0 for all x, f is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6797ef8-7860-4002-b300-3ccdaf8d2d7e",
   "metadata": {},
   "source": [
    "### 3. Finding x which minimizes f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d395923-dda2-4587-a70a-c8ec49eaa9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: -0.3958010277871996\n",
      " hess_inv: array([[0.08829298]])\n",
      "      jac: array([2.80886889e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 14\n",
      "      nit: 5\n",
      "     njev: 7\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-0.95393675])\n",
      "\n",
      "x value that minimizes the function: [-0.95393675]\n",
      "Minimum value of the function: -0.3958010277871996\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (1/12)*x**12 + (1/6)*x**6 + x + np.exp(x)\n",
    "\n",
    "x0 = [0]\n",
    "\n",
    "result = minimize(f, x0)\n",
    "print(result)\n",
    "\n",
    "print(\"\\nx value that minimizes the function:\", result.x)\n",
    "print(\"Minimum value of the function:\", result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f451a3-96b7-4bdf-9904-b4998693b821",
   "metadata": {},
   "source": [
    "### 4. Newton's Method for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be597b1-8381-4dd2-b448-638d620ff753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.694508188258762\n",
      "2: -0.2270877325345032\n",
      "3: -2.44426149039445\n",
      "4: -2.2215019268305984\n",
      "5: -2.0186691336289804\n",
      "6: -1.833777392507754\n",
      "7: -1.6649552546536384\n",
      "8: -1.5104634742402345\n",
      "9: -1.3688476977630728\n",
      "10: -1.239495346053174\n",
      "11: -1.1242382120217118\n",
      "12: -1.0306779931062928\n",
      "13: -0.9731765702068925\n",
      "14: -0.9553191468040753\n",
      "15: -0.9539444224713599\n"
     ]
    }
   ],
   "source": [
    "def f_prime(x):\n",
    "    return x**11 + x**5 + 1 + np.exp(x)\n",
    "\n",
    "def f_prime_prime(x):\n",
    "    return (11)*x**10 + (5)*x**4 + np.exp(x)\n",
    "\n",
    "x0 = 1\n",
    "n = 15\n",
    "\n",
    "for i in range(n):\n",
    "    x = x0 - f_prime(x0)/f_prime_prime(x0)\n",
    "    print(f\"{i+1}: {x}\")\n",
    "    x0 = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5dc6e5-ffd0-46c2-b619-ded4cfb700ec",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent with Armijo Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37bfcb4-b2de-48b1-b8c3-304c0948355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.2852147714426194\n",
      "2: -0.8807532549752439\n",
      "3: -0.9205706188822561\n",
      "4: -0.9414950741591653\n",
      "5: -0.9499362176638271\n",
      "6: -0.952740237579373\n",
      "7: -0.9535881979257744\n",
      "8: -0.9538361524907042\n",
      "9: -0.9539079058230353\n",
      "10: -0.9539286062017607\n",
      "11: -0.9539345728122027\n",
      "12: -0.9539362921666185\n",
      "13: -0.9539367875836582\n",
      "14: -0.9539369303307255\n",
      "15: -0.9539369714609212\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (1/12)*x**12 + (1/6)*x**6 + x + np.exp(x)\n",
    "    \n",
    "def f_prime(x):\n",
    "    return x**11 + x**5 + 1 + np.exp(x)\n",
    "    \n",
    "x0 = 1\n",
    "n = 15\n",
    "\n",
    "for i in range(n):\n",
    "    t_n = 1\n",
    "    # Armijo backtracking \n",
    "    while [f(x0 - t_n*f_prime(x0))] > [f(x0) - 0.5*t_n*(f_prime(x0))**2]:\n",
    "        t_n = t_n/2\n",
    "    x = x0 - t_n*f_prime(x0)\n",
    "    print(f\"{i+1}: {x}\")\n",
    "    x0 = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8684443-2720-47e0-9b59-91fefbb93d4d",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "### 1. Finding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2896c16-c4f3-4206-9ccb-dce7861eb923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[6*x1 - 4*x2 + exp(x1)], [-4*x1 + 10*x2]])\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = sp.symbols('x1 x2')\n",
    "f = sp.exp(x1) + 3*x1**2 + 5*x2**2 - 4*x1*x2 \n",
    "gradient = sp.Matrix([sp.diff(f, var) for var in (x1, x2)])\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf63f5e-db0b-447a-9efc-90915293b9c6",
   "metadata": {},
   "source": [
    "## 2. Finding Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7d7f09-6e0b-4904-8a4b-de8543e7780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[exp(x1) + 6, -4], [-4, 10]])\n",
      "Eigen values of the hessian: {-sqrt(exp(2*x1) - 8*exp(x1) + 80)/2 + exp(x1)/2 + 8: 1, sqrt(exp(2*x1) - 8*exp(x1) + 80)/2 + exp(x1)/2 + 8: 1}\n"
     ]
    }
   ],
   "source": [
    "f_xx = sp.diff(f, x1, x1)  # Second derivative with respect to x1\n",
    "f_xy = sp.diff(f, x1, x2)  # Mixed partial derivative x1 then x2\n",
    "f_yx = sp.diff(f, x2, x1)  # Mixed partial derivative x2 then x1\n",
    "f_yy = sp.diff(f, x2, x2)  # Second derivative with respect to x2\n",
    "\n",
    "hessian = sp.Matrix([[f_xx, f_xy], [f_yx, f_yy]])\n",
    "print(hessian)\n",
    "\n",
    "eigenval = hessian.eigenvals()\n",
    "print(f\"Eigen values of the hessian: {eigenval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3d07c-27a2-443f-bebe-07822fae0427",
   "metadata": {},
   "source": [
    "Because the eigen values of the hessian are both non-negative (the first term has a negative square root but exp(x1)/2 + 8 makes sure its positive for all x1) and the 2nd eigen value has no negative values (outside the sqrt and sqrt has to always be positive for real numbers), thus the eigen values are always positive for all x1. Because the eigen values of the hessian are non-negative, f''(x) is >= 0 for all (x1, x2), and so f is convex. Because f is convex, any stationary point is a global minimizer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ec5a8-81f1-4e87-8ba3-9642e0b5e17a",
   "metadata": {},
   "source": [
    "### 3. Find optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398476fd-27be-4df3-97be-d2e34b4262f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 0.906371315845204\n",
      " hess_inv: array([[0.18880106, 0.07622719],\n",
      "       [0.07622719, 0.13049609]])\n",
      "      jac: array([-7.34627247e-06,  3.47197056e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 18\n",
      "      nit: 3\n",
      "     njev: 6\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-0.18827173, -0.07530835])\n",
      "\n",
      "Optimal solution: [-0.18827173 -0.07530835]\n",
      "Minimum value of the function: 0.906371315845204\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.exp(x[0]) + 3*x[0]**2 + 5*x[1]**2 - 4*x[0]*x[1] \n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([0, 0])\n",
    "\n",
    "result = minimize(f, x0, method='BFGS')\n",
    "print(result)\n",
    "\n",
    "print(\"\\nOptimal solution:\", result.x)\n",
    "print(\"Minimum value of the function:\", result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9f424-96b8-4047-a076-f63a72175283",
   "metadata": {},
   "source": [
    "### 4. Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b918437d-36a6-4712-a5dc-1406b57e7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \n",
      "[6.98968298 2.79587319]\n",
      "1192.859804544672\n",
      "\n",
      "2: \n",
      "[5.9654995 2.3861998]\n",
      "468.03946481578765\n",
      "\n",
      "3: \n",
      "[4.910068  1.9640272]\n",
      "188.68792692258776\n",
      "\n",
      "4: \n",
      "[3.78722282 1.51488913]\n",
      "75.68838777990014\n",
      "\n",
      "5: \n",
      "[2.53453676 1.0138147 ]\n",
      "26.74311619345058\n",
      "\n",
      "6: \n",
      "[1.13760975 0.4550439 ]\n",
      "5.96644658227158\n",
      "\n",
      "7: \n",
      "[0.05708595 0.02283438]\n",
      "1.0659161753875501\n",
      "\n",
      "8: \n",
      "[-0.18288213 -0.07315285]\n",
      "0.9064472415660323\n",
      "\n",
      "9: \n",
      "[-0.18826827 -0.07530731]\n",
      "0.9063713158550559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.exp(x[0]) + 3*x[0]**2 + 5*x[1]**2 - 4*x[0]*x[1]\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([np.exp(x[0]) + 6*x[0] - 4*x[1], 10*x[1] - 4*x[0]])\n",
    "\n",
    "def hessian(x):\n",
    "    return np.array([[np.exp(x[0]) + 6, -4], [-4, 10]])\n",
    "\n",
    "x0 = np.array([8, 1])\n",
    "f_x = 100\n",
    "i = 1\n",
    "\n",
    "while np.abs(f_x - 0.906371315845204) > 1e-6:\n",
    "    hess_inv = np.linalg.inv(hessian(x0))\n",
    "    x = x0 - hess_inv.dot(gradient(x0))\n",
    "    f_x = f(x)\n",
    "    x0 = x\n",
    "    print(f\"{i}: \\n{x}\")\n",
    "    print(f\"{f_x}\\n\")\n",
    "    i += 1\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eafcc2-ed3e-452e-952f-efe551c985a8",
   "metadata": {},
   "source": [
    "It took 9 iterations for the function value to be within 10^-6 of the optimal value from part 3 using Newton's Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a16f5-54ea-46b7-bb5b-3f61ab272093",
   "metadata": {},
   "source": [
    "### 5. Gradient descent with Armijo Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d1fafe0-f716-4a72-95ec-36a0349345e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \n",
      "[-15.63248427   1.171875  ]\n",
      "813.2674189822978\n",
      "\n",
      "2: \n",
      "[-3.32218359 -8.10921089]\n",
      "254.18214210213694\n",
      "\n",
      "3: \n",
      "[-4.88966059  0.36621093]\n",
      "79.56700672771643\n",
      "\n",
      "4: \n",
      "[-1.04025018 -2.53638303]\n",
      "25.21203024269087\n",
      "\n",
      "5: \n",
      "[-1.57242484  0.11397067]\n",
      "8.406888750519727\n",
      "\n",
      "6: \n",
      "[-0.36206354 -0.81470509]\n",
      "3.228330008534434\n",
      "\n",
      "7: \n",
      "[-0.58489819  0.0226445 ]\n",
      "1.6390230536301567\n",
      "\n",
      "8: \n",
      "[-0.20454762 -0.29811022]\n",
      "1.140972693798533\n",
      "\n",
      "9: \n",
      "[-0.30206901 -0.02774625]\n",
      "0.98334824152265\n",
      "\n",
      "10: \n",
      "[-0.24193513 -0.0859221 ]\n",
      "0.9144676673275105\n",
      "\n",
      "11: \n",
      "[-0.20158322 -0.09948704]\n",
      "0.9086116161414591\n",
      "\n",
      "12: \n",
      "[-0.20231877 -0.07591985]\n",
      "0.9070122336322594\n",
      "\n",
      "13: \n",
      "[-0.19064393 -0.08217942]\n",
      "0.9065613808271139\n",
      "\n",
      "14: \n",
      "[-0.19205404 -0.07477711]\n",
      "0.9064296296769936\n",
      "\n",
      "15: \n",
      "[-0.18855985 -0.07733274]\n",
      "0.9063897522735437\n",
      "\n",
      "16: \n",
      "[-0.1893252  -0.07494674]\n",
      "0.9063772914066194\n",
      "\n",
      "17: \n",
      "[-0.18824434 -0.07592592]\n",
      "0.9063732906964491\n",
      "\n",
      "18: \n",
      "[-0.18857558 -0.07514069]\n",
      "0.9063719781915331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.exp(x[0]) + 3*x[0]**2 + 5*x[1]**2 - 4*x[0]*x[1]\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([np.exp(x[0]) + 6*x[0] - 4*x[1], 10*x[1] - 4*x[0]])\n",
    "\n",
    "x0 = np.array([8, 1])\n",
    "f_x = 100\n",
    "i = 1\n",
    "\n",
    "while np.abs(f_x - 0.906371315845204) > 1e-6:\n",
    "    t_n = 1 \n",
    "    while (f(x0 - t_n * gradient(x0)) > f(x0) - 0.5 * t_n * (gradient(x0))**2).all():\n",
    "        t_n = t_n/2\n",
    "    x = x0 - t_n * gradient(x0)\n",
    "    f_x = f(x)\n",
    "    print(f\"{i}: \\n{x}\")\n",
    "    print(f\"{f_x}\\n\")\n",
    "    x0 = x\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4d93b-9b01-409f-a94d-b906e29b9653",
   "metadata": {},
   "source": [
    "It took 18 iterations for the function value to be within 10^-6 of the optimal value from part 3 using gradient descent with Armijo backtracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
